{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook aims to build a Artificial Neural Network (NN) in order to beat a Ordinary Least Squares Regression (OLS).\n",
    "\n",
    "The comparison variable shall be the Mean Average Percentage Error (MAPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from datetime import datetime\n",
    "from MLP import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "na = '<NA>'\n",
    "\n",
    "df = pd.read_csv(r'../data_file/selected_data.csv')\n",
    "df = df.replace(na, np.nan).drop(columns = [df.columns[0]])\n",
    "\n",
    "obj_type, rooms, surface, zip_code_2_digits, zip_code_3_digits, canton, year_built, year_renovated, distance_to_station, lon, lat, price_square_metres, price = df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- frame_to_numeric aims to force each string-formated value to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_to_numeric(frame):\n",
    "    \"\"\"transforms every value of a data frame to numeric if possible\"\"\"\n",
    "    for column in frame.columns:\n",
    "        try:\n",
    "            frame.loc[:, column] = pd.to_numeric(frame.loc[:, column])\n",
    "        except:\n",
    "            continue #nothing toDo here\n",
    "    return frame\n",
    "\n",
    "df = frame_to_numeric(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add_nearest_rooms_by_surface & add_surface_mean_by_nearest_rooms aims to fill missing surface and rooms values in order to have a larger dataset to train the LRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rooms_surface_mean(df):\n",
    "    df = frame_to_numeric(df.loc[:, [rooms, surface]].dropna())\n",
    "    return df.groupby(rooms).mean()\n",
    "\n",
    "def add_rooms(row, df_rooms_mean):\n",
    "    return df_rooms_mean.loc[row[rooms]]\n",
    "\n",
    "def add_nearest_rooms_by_surface(df):\n",
    "    \"\"\"completes the dataframe rooms by the nearest neighbour based on the surface mean distance\n",
    "    and the surface of the missing rooms objects\"\"\"\n",
    "    df = df.reset_index(drop = True)\n",
    "    df_rooms_mean = rooms_surface_mean(df)\n",
    "    df_rooms_missing = df[df.loc[:, rooms].isna()].loc[:, [surface]]\n",
    "    dist = cdist(df_rooms_missing, df_rooms_mean) #computes the distance between each pair of surface\n",
    "    idx = np.argsort(dist) #gets sorted index (most left = lowest distance index)\n",
    "    rooms_class = idx[:,0] #gets the room class (index)\n",
    "    df_rooms_missing.loc[:, rooms] = rooms_class\n",
    "    df_rooms_mean = df_rooms_mean.reset_index()\n",
    "    df_rooms_missing.loc[:, rooms] = df_rooms_missing.apply(lambda row: add_rooms(row, df_rooms_mean), axis = 1)\n",
    "    df.loc[df_rooms_missing.index, rooms] = df_rooms_missing.loc[:, rooms] #appends the missing rooms to the real data-frame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_surface(row, df_rooms_mean):\n",
    "    return df_rooms_mean.loc[row[surface]]\n",
    "\n",
    "def add_surface_mean_by_nearest_rooms(df):\n",
    "    \"\"\"completes the dataframe surface by the nearest neighbour surface mean based on the room distance\n",
    "    of the missing surface objects\"\"\"\n",
    "    df = df.reset_index(drop = True)\n",
    "    df_rooms_mean = rooms_surface_mean(df).reset_index()\n",
    "    df_surface_missing = df[df.loc[:, surface].isna()].loc[:, [rooms]]\n",
    "    dist = cdist(df_surface_missing, df_rooms_mean.loc[:, [rooms]]) #computes the distance between each pair of rooms\n",
    "    idx = np.argsort(dist) #gets sorted index (most left = lowest distance index)\n",
    "    surface_class = idx[:,0] #gets the surface class (index)\n",
    "    df_surface_missing.loc[:, surface] = surface_class\n",
    "    df_surface_missing.loc[:, surface] = df_surface_missing.apply(lambda row: add_surface(row, df_rooms_mean), axis = 1)\n",
    "    df.loc[df_surface_missing.index, surface] = df_surface_missing.loc[:, surface]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- handle_missing_numeric_feature completes missing values of a dataframe with an additional feature which has a value 1 if the value was not missing and 0 if it was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_numeric_feature(df, feature, na):\n",
    "    \"\"\"this function completes the missing vlaues of a data-frame with the median of the given numeric feature\n",
    "    it adds a new column `feature`+'_'+`na` at the end of the feature index,\n",
    "    which is going to be '1' if the value of the data object is missing and '0' otherwise \n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    na_index = pd.Index(df_copy.columns).get_loc(feature) + 1\n",
    "    column = df_copy.loc[:, feature]\n",
    "    column = column.replace(np.nan, na, regex = True)\n",
    "    artificial_column = [1. if x == na else 0. for x in column] #comprehensions\n",
    "    df.insert(na_index, feature+'_'+na, artificial_column)\n",
    "    \n",
    "    na_indexes = column[column == na].index\n",
    "    feature_median = df_copy.loc[:, feature].median()\n",
    "    df.loc[na_indexes, feature] = feature_median\n",
    "    return df\n",
    "\n",
    "def missing_numerical_features_controller(df, features, na):\n",
    "    \"\"\"this function just calls the :func:`handle_missing_numeric_feature` for each given feature\"\"\"\n",
    "    for feature in features:\n",
    "        df = handle_missing_numeric_feature(df, feature, na)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- one_hot_encoding transformes catecorical features to numeric with the one hot encoding method because the NN model just works with numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-8d616d9386bc>, line 114)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-8d616d9386bc>\"\u001b[1;36m, line \u001b[1;32m114\u001b[0m\n\u001b[1;33m    df[features] = X-\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encoding(df, na, feature_name, new_encoding = False):\n",
    "    \"\"\"performs a complete one-hit encoding on the specified feature of the dataframe\n",
    "    adds a the prefix '_' to each unique encoding class\n",
    "    set `new_encoding` = True if the data are not in the same order as before!!! otherwise it returns a false encoding\n",
    "    separates automatically all missing values as it's own category named: feature_name+'_'+na\n",
    "    returns: \n",
    "    -encoded data-frame\n",
    "    -categories of the variable as an array\n",
    "    \"\"\"\n",
    "    enc = OneHotEncoder()\n",
    "    df = df.copy()\n",
    "    delimiter = feature_name + '_'\n",
    "    na_delimiter = '_' + na\n",
    "    enc_dir = './enc'\n",
    "    cat_dir = './cat'\n",
    "    enc_path = enc_dir+'/'+feature_name+'.npz'\n",
    "    cat_path = cat_dir+'/categories.pkl'\n",
    "    encoding = False\n",
    "    sparse_matrix = None\n",
    "    category = None\n",
    "    all_cat = {}\n",
    "    \n",
    "    #creates directories if not exists\n",
    "    if not os.path.exists(enc_dir):\n",
    "        encoding = True\n",
    "        os.makedirs(enc_dir)\n",
    "    if not os.path.exists(cat_dir):\n",
    "        encoding = True\n",
    "        os.makedirs(cat_dir)\n",
    "        \n",
    "    #gets the dictionary with all categories\n",
    "    try:\n",
    "        all_cat = load_obj(cat_path)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #deletes files if they exists and new_encoding is True\n",
    "    if new_encoding:\n",
    "        encoding = True\n",
    "        try:\n",
    "            os.remove(enc_path)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            del all_cat[feature_name]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    #tries to get sparse matrix & head if new_encoding == False\n",
    "    if not encoding:\n",
    "        try:\n",
    "            sparse_matrix = scipy.sparse.load_npz(enc_path)\n",
    "            #check if size of sparse_matrix matches size of the df\n",
    "            if sparse_matrix.shape[0] != df.shape[0]:\n",
    "                sparse_matrix = None\n",
    "            category = all_cat.get(feature_name)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    #perform actual encoding if necessary\n",
    "    if (sparse_matrix is None) or (category is None):\n",
    "        if df.loc[:, [feature_name]].isnull().values.any():\n",
    "            df.loc[:, feature_name] = df.loc[:, feature_name].replace(np.nan, feature_name + na_delimiter, regex = True)\n",
    "        selected_frame = delimiter + df.loc[:, feature_name].astype(str).to_frame()\n",
    "        sparse_matrix = enc.fit_transform(selected_frame)\n",
    "        category = enc.categories_[0]\n",
    "        all_cat[feature_name] = category\n",
    "        scipy.sparse.save_npz(enc_path, sparse_matrix)\n",
    "        save_obj(all_cat, cat_path)\n",
    "    \n",
    "    #add encoded matrix to the df\n",
    "    encoded_array = sparse_matrix.toarray()\n",
    "    df_enc = pd.DataFrame(data = encoded_array, columns = category)\n",
    "    category_sorted = np.sort(category)\n",
    "    df_enc = df_enc[category_sorted]\n",
    "    df = df.drop(columns = [feature_name])\n",
    "    df[category_sorted] = df_enc\n",
    "    return df, category_sorted\n",
    "\n",
    "def save_obj(obj, path):\n",
    "    if len(path) < 5:\n",
    "        path = path + '.pkl'\n",
    "    elif path[-4:] != '.pkl':\n",
    "        path = path + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(path):\n",
    "    if len(path) < 5:\n",
    "        path = path + '.pkl'\n",
    "    elif path[-4:] != '.pkl':\n",
    "        path = path + '.pkl'\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_prepared_df(df, na, target_variable = None, categorical_features = [], numerical_features = [], additional_features = [], new_encoding = False):\n",
    "    \"\"\"returns the data frame with the specified encoded features and the corresponding generated categories\n",
    "    set `new_encoding` = True if the data are not in the same order as before!!! otherwise it returns a false encoding\"\"\"\n",
    "    df_selected = df.loc[:, (numerical_features + categorical_features + additional_features)]\n",
    "    categories = {}\n",
    "    df_selected = normalize_df(df_selected, numerical_features)\n",
    "    for feature in categorical_features:\n",
    "        df_selected, categories[feature] = one_hot_encoding(df_selected, na, feature, new_encoding)\n",
    "    if target_variable is not None:\n",
    "        df_selected[target_variable] = df.loc[:, target_variable]\n",
    "    return df_selected, categories\n",
    "\n",
    "def normalize_df(df, features):\n",
    "    \"\"\"normalizes the given numerical sample\"\"\"\n",
    "    #df[features] = StandardScaler().fit_transform(df.loc[:, features])\n",
    "    X = df.loc[:, features].to_numpy()\n",
    "    X -= np.mean(X, axis = 0)\n",
    "    X /= np.std(X, axis = 0)\n",
    "    df[features] = X\n",
    "    return df\n",
    "\n",
    "def mape(y_true, y_pred): \n",
    "    \"\"\"calculates the mean absolute percentage error (MAPE) of a continuous predicted variable\"\"\"\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def load_mlp_res(path):\n",
    "    if not os.path.exists(path):\n",
    "        df_mlp = pd.DataFrame(columns = ['features', 'layers', 'learning_rate', 'alpha', 'batch_size', 'residuals', 'MAPE'])\n",
    "    else:\n",
    "        df_mlp = load_obj(path)\n",
    "    return df_mlp\n",
    "\n",
    "def select_df(df, na, selected_features, missing_values_included = False, reset_idx = True):\n",
    "    df = df.copy()\n",
    "    cols_in = set(df.columns)\n",
    "    if missing_values_included:\n",
    "        df = add_nearest_rooms_by_surface(df)\n",
    "        df = add_surface_mean_by_nearest_rooms(df)\n",
    "    else:\n",
    "        df = df.loc[:, selected_features]\n",
    "        if year_renovated in df.columns:\n",
    "            year_renovated_frame = df.loc[:, [year_renovated]]\n",
    "            df = df.drop(columns = [year_renovated])\n",
    "            df = df.dropna()\n",
    "            df[year_renovated] = year_renovated_frame.iloc[df.index, :]\n",
    "            df = handle_missing_numeric_feature(df, year_renovated, na)\n",
    "        else:\n",
    "            df = df.dropna()\n",
    "        if reset_idx:\n",
    "            df = df.reset_index(drop = True)\n",
    "    cols_out = set(df.columns)\n",
    "    add_features = list(cols_out - cols_in)\n",
    "    return df, add_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_NN(X_train, y_train, X_test, y_test, features):\n",
    "    layers, learning_rate, alpha, batch_size, max_iter, plot_error = (80,), 0.01, 0.001, 'none', 1000, True\n",
    "    mlp = MLP(layers, learning_rate = learning_rate, alpha = alpha, batch_size = batch_size, max_iter = max_iter, plot_error = plot_error)\n",
    "    X_train, y_train, X_test, y_test = X_train.to_numpy(), y_train.to_numpy().T[0], X_test.to_numpy(), y_test.to_numpy().T[0]\n",
    "    mlp.fit(X_train, y_train)\n",
    "    predicted = mlp.predict(X_test)\n",
    "        \n",
    "    m = mape(y_test, predicted)\n",
    "    residuals = y_test - predicted\n",
    "    \n",
    "    df_mlp_path = './mlp_res.pkl'\n",
    "    result = [features, layers, learning_rate, alpha, batch_size, residuals, m]\n",
    "    df_mlp = load_mlp_res(df_mlp_path)\n",
    "    series = pd.Series(result, index = df_mlp.columns)\n",
    "    df_mlp = df_mlp.append(series, ignore_index = True)\n",
    "    save_obj(df_mlp, df_mlp_path)\n",
    "    \n",
    "    mlp.store('./NN_1.pkl')\n",
    "    return df_mlp\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'select_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a5e40d4a227b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf_mlp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-a5e40d4a227b>\u001b[0m in \u001b[0;36minit\u001b[1;34m(df, na, missing_values_included)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mselected_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtarg_var\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing_values_included\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_prepared_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg_var\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_encoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdf_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg_var\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'select_df' is not defined"
     ]
    }
   ],
   "source": [
    "def init(df, na, missing_values_included = False):\n",
    "    #initialization and declaration of the variables\n",
    "    #obj_type', 'rooms', 'surface', 'zip_code_2_digits', 'canton', \n",
    "    #'year_built', 'year_renovated','distance_to_station', 'price_square_metres', 'price'\n",
    "    cat_f = [obj_type, canton, zip_code_2_digits]\n",
    "    compl_num_f = [surface, rooms, distance_to_station]\n",
    "    miss_num_f = [year_renovated, year_built]\n",
    "    targ_var = [price]\n",
    "    num_f = compl_num_f + miss_num_f\n",
    "    features = cat_f + compl_num_f + miss_num_f\n",
    "    selected_features = features + targ_var\n",
    "    \n",
    "    df, add_f = select_df(df, na, selected_features, missing_values_included)\n",
    "    df, categories = get_prepared_df(df, na, targ_var[0], cat_f, num_f, add_f, new_encoding = True)\n",
    "    df_y = df.loc[:, targ_var]\n",
    "    df_X = df.drop(targ_var, axis = 1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size = 0.25, random_state = 42)\n",
    "    df_mlp = launch_NN(X_train, y_train, X_test, y_test, features)\n",
    "    return df_mlp\n",
    "    \n",
    "init(df, na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result 13203 Rental Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
